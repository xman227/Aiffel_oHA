{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb4eb39",
   "metadata": {},
   "source": [
    "# [E-04] To make lyricist "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69470caf",
   "metadata": {},
   "source": [
    "## Contexts\n",
    "\n",
    "### 1. READY\n",
    "    1-1 ì˜¤ëŠ˜ì˜ Expì™€ Rubric  \n",
    "    1-2 ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬  \n",
    "\n",
    "### 2. GAME\n",
    "    2-1. ë°ì´í„° ì½ì–´ì˜¤ê¸°  \n",
    "    2-2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        2-2-1. Tokenize (ë¬¸ì¥ì˜ ì •í˜•í™”)\n",
    "        2-2-2. Tensor (ë¦¬ìŠ¤íŠ¸ ê°ì²´ë¥¼ í…ì„œ ê°ì²´ë¡œ ë³€í™˜)\n",
    "        2-2-3. ë°ì´í„° ë¶„ë¦¬ (Train, Validation)\n",
    "        2-2-4. ë°ì´í„°ì…‹ í™” (í…ì„œ ê°ì²´ë¥¼ dataset ê°ì²´ë¡œ ë³€í™˜)\n",
    "    2-3. ëª¨ë¸ í•™ìŠµ  \n",
    "    2-4. ë°ì´í„° í‰ê°€   \n",
    "\n",
    "### 3. POTG\n",
    "    3-1. ì†Œê°(POTG)  \n",
    "    3-2. ì–´ë ¤ì› ë˜ ì ê³¼ ê·¹ë³µë°©ì•ˆ  \n",
    "    3-3. ì¶”í›„  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9e9b7",
   "metadata": {},
   "source": [
    "## 1. READY\n",
    "\n",
    "   ### 1-1. ì˜¤ëŠ˜ì˜ EXP ì™€ Rubric\n",
    "\n",
    "ì˜¤ëŠ˜ì˜ EXP ë‚´ìš©ì€ NLP (Natural Language Processing) ê¸°ìˆ ì„ ì´ìš©í•œ ì¸ê³µì§€ëŠ¥ ì‘ì‚¬ ëª¨ë¸ ìƒì„±ì´ë‹¤.\n",
    "\n",
    "- ë°ì´í„° : ë¯¸êµ­ ìœ ëª… ì•„í‹°ìŠ¤íŠ¸ 50ëª…ë“¤ì˜ ê³¡ ê°€ì‚¬ì •ë³´ë¥¼ ë‹´ì€ í…ìŠ¤íŠ¸ íŒŒì¼\n",
    "<img src=\"./img/lyric.PNG\" width=\"200px\"></img>\n",
    "\n",
    "\n",
    "- ëª¨ë¸ : tensor - Adam\n",
    "\n",
    "- rubric ì œì‹œ\n",
    "\n",
    "|í‰ê°€ë¬¸í•­|ìƒì„¸ê¸°ì¤€|\n",
    "|---|---|\n",
    "|1. ê°€ì‚¬ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ê°€?|í…ìŠ¤íŠ¸ ì œë„ˆë ˆì´ì…˜ ê²°ê³¼ê°€ ê·¸ëŸ´ë“¯í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„œë˜ëŠ”ì§€|\n",
    "|2. ë°ì´í„°ì˜ ì „ì²˜ë¦¬/ ë°ì´í„°ì…‹ êµ¬ì„± ê³¼ì •ì´ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ëŠ”ê°€?| íŠ¹ìˆ˜ë¬¸ì ì œê±°, í† í¬ë‚˜ì´ì € ìƒì„±, íŒ¨ë”©ì²˜ë¦¬ ë“±ì˜ ê³¼ì •ì´ ë¹ ì§ì—†ì´ ì§„í–‰ë˜ëŠ”ì§€|\n",
    "|3. í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì´ ì˜ í•™ìŠµë˜ì—ˆëŠ”ì§€|í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì˜ validation lossê°€ 2.2 ì´í•˜ì¸ì§€|\n",
    "\n",
    "### 1-2. ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b351bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf #tf.keras.preprocessing.text.Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d757f",
   "metadata": {},
   "source": [
    "- glob : íŒŒì¼ í•¸ë“¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- os : ê²½ë¡œ ì„¤ì • ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- re : ì •ê·œí‘œí˜„ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- tf : ì¸ê³µì§€ëŠ¥ ëª¨ë¸ ìƒì„±, í•™ìŠµ, ì €ì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬  \n",
    "  tf.keras.preprocessing.text.Tokenizer :(í† í°í™” / ë‹¨ì–´ì‚¬ì „ ìƒì„± / ìˆ«ìë³€í™˜) ë²¡í„°í™”\n",
    "- train_test_split : ëª¨ì€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- np / pandas : ë°ì´í„° êµ¬ì„±ì„ ì´ë£¨ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f2713",
   "metadata": {},
   "source": [
    "# 2. GAME\n",
    "## 2-1. ë°ì´í„° ì½ì–´ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8821aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel//Study/Exp_4_lyric/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ìŠµë‹ˆë‹¤.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d506f",
   "metadata": {},
   "source": [
    "ì ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë¬¸ì¥ë³„ë¡œ ì­‰ strë°ì´í„°ê°€ ë“¤ì–´ê°”ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50a55f",
   "metadata": {},
   "source": [
    "## 2-2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "### 2-2-1. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3140f1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Now', \"I've\", 'heard', 'there', 'was', 'a', 'secret', 'chord']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1 ì†Œë¬¸ìí™”\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence) # 2 íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3 ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence) # 4 ì•ŒíŒŒë²³, ?.!,Â¿ê°€ ì•„ë‹Œ ëª¨ë“  ë¬¸ìë¥¼ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ\n",
    "    sentence = sentence.strip() # 5 ì–‘ìª½ ê³µë°± ì§€ìš°ê¸°\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6 \n",
    "    return sentence\n",
    "\n",
    "raw_corpus[0].split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc741d87",
   "metadata": {},
   "source": [
    "preprocess_sentence() í•¨ìˆ˜ë¡œ ë°ì´í„°ë¥¼ ì •í˜•í™” ì‹œì¼œì•¼ í•œë‹¤.\n",
    "\n",
    "* preprocess_sentence() ëŠ”  \n",
    "  - ë¬¸ì¥ë¶€í˜¸ë¥¼ ë„ì›Œ ë†“ê¸°\n",
    "  - ì „ë¶€ ì†Œë¬¸ìí™”\n",
    "  - íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "  - start , end ì¶”ê°€  \n",
    "  \n",
    "ë¥¼ í†µí•´ ëª¨ë“  ë¬¸ì¥ì„ ë™ì¼í•œ ê·œì¹™ì„±ì„ ë ê²Œ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d3a050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19736 ê°œëŠ” ë„ˆë¬´ ê¸¸ì–´ì„œ ì œì™¸\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156013"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ì—¬ê¸°ì— ì •ì œëœ ë¬¸ì¥ì„ ëª¨ì„ê²ë‹ˆë‹¤\n",
    "corpus = []\n",
    "\n",
    "\n",
    "excepted = 0  # ì œì™¸ë˜ëŠ” ì–‘\n",
    "for sentence in raw_corpus:\n",
    "    # ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # ì •ì œë¥¼ í•˜ê³  ë‹´ì•„ì£¼ì„¸ìš”\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    if len(preprocessed_sentence.split()) > 15:\n",
    "        excepted += 1\n",
    "        continue\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "print(excepted, 'ê°œëŠ” ë„ˆë¬´ ê¸¸ì–´ì„œ ì œì™¸')\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03678849",
   "metadata": {},
   "source": [
    "\n",
    "ë§Œë“  í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ê¸° ì „\n",
    "- ì•„ë¬´ê²ƒë„ ì—†ëŠ” ì¤„\n",
    "- : ë¡œ ëë‚˜ëŠ” ë¬¸ì¥(ê°€ìˆ˜ë‚˜ ìƒëŒ€ê°€ìˆ˜ì˜ ì´ë¦„ì´ ìˆë‹¤ê³  ì¶”ì¸¡)  \n",
    "\n",
    "ì„ ì œê±°í•œ ë‹¤ìŒ ë©”ì„œë“œë¡œ ì •í˜•í™”ë¥¼ ì‹œì¼°ë‹¤, ê·¸ë¦¬ê³  ì •ì œëœ ë¬¸ì¥ ì¤‘  \n",
    "ë‹¨ì–´ê°€ 15ê°œë¥¼ ë„˜ëŠ” ë¬¸ì¥ë“¤(start, end í¬í•¨) ì„ ì œí•˜ê³  ë¬¸ì¥ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883c0af",
   "metadata": {},
   "source": [
    "### 2-2-2. Tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0b539",
   "metadata": {},
   "source": [
    "ìš°ë¦¬ì˜ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ìš°ë¦¬ì˜ tokenizer ë°ì´í„°ë¥¼  \n",
    "tensor ê°ì²´ë¡œ ë³€í™˜ì‹œì¼œì£¼ì–´ì•¼ í•œë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e305e780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f22905e3f40>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤ë‹ˆë‹¤\n",
    "    # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
    "    # ë¬¸ì¥ ì•ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶”ê³  ì‹¶ë‹¤ë©´ padding='pre'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efd040c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   50,    4,   95,  303,   62,   53,    9,  946, 6263,    3,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455e041",
   "metadata": {},
   "source": [
    "ê·¸ ê°ì²´ì™€ í•¨ê»˜ ì“°ì´ëŠ” ë¬¸ì¥ì‚¬ì „(12000ê°œë¡œ êµ¬ì„±)ì„ Tokenizer ë©”ì„œë“œë¥¼ í†µí•´ ë¶ˆë €ê³ ,  \n",
    "ìš°ë¦¬ê°€ ëª¨ì€ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í†µí•´ í•„ìš”í•œ ë¬¸ì¥ì„ ì‚¬ì „ì— êµ¬ì¶•í•˜ì˜€ë‹¤.\n",
    "\n",
    "ì¶”ê°€ë¡œ\n",
    "- tokenizer.texts_to_sequences ë©”ì„œë“œë¥¼ í†µí•´ ìš°ë¦¬ì˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œí™” í–ˆë‹¤.  \n",
    "- tf.keras.preprocessing.sequence.pad_sequencesë©”ì„œë“œë¥¼ í†µí•´ ë¬¸ì¥ë³„ token ìˆ˜ë¥¼ ë§ì¶˜ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53c867",
   "metadata": {},
   "source": [
    "í…ì„œì˜ src , target ë°ì´í„°ë¥¼ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "src ì¸ìŠ¤í„´ìŠ¤ëŠ” ì—”ë”©ì´ ì—†ëŠ” ì‹œì‘ ë°ì´í„° (ë¬¸ì œì§€) ì´ê³   \n",
    "tgt ì¸ìŠ¤í„´ìŠ¤ëŠ” ìŠ¤íƒ€íŠ¸ê°€ ì—†ëŠ” ë§ˆë¬´ë¦¬ ë°ì´í„° (ë‹µì•ˆì§€) ì´ë‹¤.\n",
    "\n",
    "ì´ ë‘ ë°ì´í„°ë¥¼ ì—°ê²°ì§€ìœ¼ë©° ë‹¤ìŒì— ì˜¬ ë¬¸ì¥ì„ ì˜ˆì¸¡í•œë‹¤.\n",
    "\n",
    "**ì´ê²ƒì´ RNN ì˜ ê¸°ë³¸ ë©”ì»¤ë‹ˆì¦˜ì´ë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58375889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1760a",
   "metadata": {},
   "source": [
    "### 2-2-3. ë°ì´í„°ì…‹ì„ train, validation ê°’ìœ¼ë¡œ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354cf05",
   "metadata": {},
   "source": [
    "NLP ëŠ” ëª©ì ì„±(ìƒˆë¡œìš´ ì‘ì‚¬ë¥¼ í•˜ë ¤ëŠ” ì˜ë„) ì— ë”°ë¼ test ê°’ìœ¼ë¡œ ì¸¡ì •í•˜ê¸°ê°€ ì• ë§¤í•˜ë‹¤.  \n",
    "ê·¸ëŸ¬ë¯€ë¡œ ìš°ë¦¬ì—ê²Œ í•„ìš”í•œ ë°ì´í„° êµ¬ì„±ì€\n",
    "\n",
    "- ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” Train ë°ì´í„°\n",
    "- ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ë©° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ì£¼ëŠ” Validation ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84d0838",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6222e0",
   "metadata": {},
   "source": [
    "sklearn ëª¨ë“ˆì˜ train_test_split ë©”ì„œë“œë¡œ 0.2ì˜ ë¹„ìœ¨ë¡œ ë¶„ë¦¬í–ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae29071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d5a9d",
   "metadata": {},
   "source": [
    "ìœ„ì™€ ê°™ì´ 14ê°œì˜ tokenì„ ê°€ì§„ 124810ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5b5e5",
   "metadata": {},
   "source": [
    "### 2-2-4. dataset\n",
    "\n",
    "ëª¨ë¸ í•™ìŠµ ì§ì „ ê·¸ ëª¨ë¸ì— ë§ë„ë¡ ë°ì´í„°ì— ì¼ì • íŒŒë¼ë¯¸í„° ê°’ì„ ì„¤ì •í•˜ê³  ë³€í™˜ì‹œì¼œì•¼ í•œë‹¤.\n",
    "\n",
    "1. buffer size : ë„£ì„ ë°ì´í„°ì˜ ì–‘ (124810)\n",
    "2. batch size : ìˆ˜í–‰í•  ë¯¸ë‹ˆë°°ì¹˜ì˜ í¬ê¸° (256ê°œì”© ë¬¶ì„ ê²ƒì´ë‹¤)\n",
    "3. steps per epoch : epochë§ˆë‹¤ í•™ìŠµí•  íšŸìˆ˜ = (ë²„í¼ì‚¬ì´ì¦ˆë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë‚˜ëˆˆ ë§Œí¼)\n",
    "\n",
    "* tf.data.Dataset.from_tensor_slices ë©”ì„œë“œë¥¼ í†µí•´  \n",
    "tensorì—ì„œ dataset ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•œë‹¤.\n",
    "\n",
    "\n",
    "* shuffle ë©”ì„œë“œì™€ batch ë©”ì„œë“œë¡œ ì„¤ì •í•œ íŒŒë¼ë¯¸í„°ê°’ì„ ë„£ì–´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82827eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487\n",
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "print(steps_per_epoch)\n",
    "\n",
    " # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 12000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)) #ì´ê±¸ë¡œ tensorì˜ train ë°ì´í„°ë¥¼ tf.data.dataset ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í• ê±°ì„\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)) #ì´ê±¸ë¡œ tensorì˜ validation ë°ì´í„°ë¥¼ tf.data.dataset ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í• ê±°ì„\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578334d",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2368c",
   "metadata": {},
   "source": [
    "## 2-3. ëª¨ë¸ í•™ìŠµ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "566e0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c259e7f",
   "metadata": {},
   "source": [
    "TextGenerator í´ë˜ìŠ¤ëŠ” ìš°ë¦¬ê°€ ë§Œë“¤ ëª¨ë¸ì˜ ì‘ë™ì›ë¦¬ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ .  \n",
    "ë‚´ë¶€ì—ëŠ” ì„ë² ë”© ë ˆì´ì–´, 2ê°œì˜ RNN ë ˆì´ì–´, 1ê°œì˜ Dense ë ˆì´ì–´ê°€ ìˆë‹¤.  \n",
    "\n",
    "- embedding size : ì›Œë“œ ë²¡í„°ì˜ ì°¨ì› ìˆ˜  \n",
    "- hidden size : LSTMë ˆì´ì–´ì˜ hidden state ì°¨ì› ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53779e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_31/1230560086.py:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Epoch 1/10\n",
      "487/487 [==============================] - 289s 518ms/step - loss: 3.2777 - val_loss: 2.9099\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 262s 537ms/step - loss: 2.7318 - val_loss: 2.6511\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 262s 538ms/step - loss: 2.4125 - val_loss: 2.4685\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 262s 537ms/step - loss: 2.1014 - val_loss: 2.3304\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 261s 536ms/step - loss: 1.8100 - val_loss: 2.2380\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 261s 536ms/step - loss: 1.5535 - val_loss: 2.1693\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 262s 537ms/step - loss: 1.3446 - val_loss: 2.1385\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 261s 536ms/step - loss: 1.1872 - val_loss: 2.1399\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 261s 536ms/step - loss: 1.0818 - val_loss: 2.1555\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 262s 538ms/step - loss: 1.0219 - val_loss: 2.1767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21ec2459d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d2dda",
   "metadata": {},
   "source": [
    "í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤  \n",
    "ìµœì¢… ê²°ê³¼ë¡œëŠ” loss : 1.34, val-loss : 2.14 ë¥¼ ì±„íƒí•  ìˆ˜ ìˆë‹¤! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693538d",
   "metadata": {},
   "source": [
    "### 2-4. ë°ì´í„° í‰ê°€\n",
    "ë§í•œ ë°”ì™€ ê°™ì´ NLP ì˜ ê²½ìš°ì—ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë”°ë¡œ ì—†ê¸°ì—  \n",
    "ë§Œë“¤ì–´ì§„ ëª¨ë¸ë¡œ ì§ì ‘ ê¸€ì„ ì‘ì„±í•˜ì—¬ ë§ˆìŒì— ë“œëŠ”ì§€ë¥¼ í™•ì¸í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d61e3a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizerë¥¼ ì´ìš©í•´ word indexë¥¼ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤ \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190a503",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì—ì„œëŠ” tensor ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—  \n",
    "ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ê³  ë°ì´í„°ë¥¼ ë¹¼ì˜¬ ë•Œ í•­ìƒ ë°ì´í„°ê°ì²´ ì „í™˜ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. \n",
    "\n",
    "ìœ„ëŠ” ê·¸ ê¸°ëŠ¥ê³¼ í•¨ê»˜ ìµœëŒ€ 20ë‹¨ì–´ê¹Œì§€ ë‚˜ì˜¤ë„ë¡ ì„¤ì •í•œ í•¨ìˆ˜ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc44cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so much , so o o <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5fa94",
   "metadata": {},
   "source": [
    "ë„ˆë¬´ ë¡œë§¨í‹±í•˜ë‹¤, ì‘ì‚¬ì— ë§ê²Œ ì‘ê³¡í•˜ê³  ì‹¶ì€ ìš•êµ¬ê°€ ìƒ˜ì†ŸëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df8833bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a monster <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f38aae",
   "metadata": {},
   "source": [
    "ì–´ë–¤ ì˜ë¯¸ì˜ ê´´ë¬¼ì¼ì§€ì— ëŒ€í•œ ê¶ê¸ˆì¦ì´ ë“±ì„ íƒ€ê³  íë¥¸ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "902fa74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3c682",
   "metadata": {},
   "source": [
    "ë‚˜ë„ ë‚´ ê³ì„ ë§´ë„ëŠ” ê·¸ë…€ê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2575a248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c581a228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> finished work , i m gonna watch this motherfuckin thing <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> finished work ,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d48a3a",
   "metadata": {},
   "source": [
    "ë‚œ ë‹¤ í–ˆê³ , ì´ì œ ë¹Œì–´ë¨¹ì„ ë„·í”Œë¦­ìŠ¤ ë³´ëŸ¬ê°„ë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f15f99",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78dda4",
   "metadata": {},
   "source": [
    "\n",
    "## 3. POTG\n",
    "\n",
    "### 3-1. ì†Œê°(POTG)  \n",
    "#### \" ğŸ™„ ë‚˜, NLPì— ì¬ëŠ¥ì´ ìˆì–´ë²„ë¦´ì§€ë„..?\" \n",
    "CVì™€ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ë°©ì‹ì— í¥ë¯¸ë¥¼ ëŠê¼ˆìŠµë‹ˆë‹¤! ê·¸ëŸ°ë° ê²°êµ­ ê°€ì¥ ì˜ ë‚˜ì˜¬ ê²ƒ ê°™ì€ ë§ì„ í†µê³„ì ìœ¼ë¡œ ì°¾ì•„ ë¶™ì´ëŠ” ê±°ë¼ë©´ ë…íŠ¹í•˜ê³  ì°½ì˜ì ì¸ ê°€ì‚¬ëŠ” ë‚˜ì˜¤ê¸° í˜ë“  ê²ƒì´ ì•„ë‹ê¹Œìš”?\n",
    "\n",
    "### 3-2. ì–´ë ¤ì› ë˜ ì ê³¼ ê·¹ë³µë°©ì•ˆ  \n",
    "\n",
    "#### 3-2-1. NLP ëª¨ë¸ êµ¬ì¡°ì˜ ì‹¬ë„ì—†ëŠ” ì´í•´\n",
    "\n",
    "ìš°ë¦¬ê°€ ì˜¤ëŠ˜ ë§Œë“  ëª¨ë¸ì€ ì„ë² ë”© ë ˆì´ì–´ í•œ ê°œ, LSTM ë ˆì´ì–´ ë‘ ê°œ,  \n",
    "Danse ë ˆì´ì–´ í•œ ê°œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"./img/rnn2.PNG\"></img>\n",
    "\n",
    "í•˜ì§€ë§Œ ì €ëŠ” ì•„ì§ RNN ì´ ë­”ì§€, ì™œ DANSE ëª¨ë¸ì„ LInear ë¼ í•˜ëŠ”ì§€,  \n",
    "ëª¨ë¥´ëŠ” ê²ƒ íˆ¬ì„±ì…ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì„ ì´í•´í•˜ì§€ ëª»í•œ ì±„ í•™ìŠµì„ ì‹œí‚¤ë ¤ë‹ˆ, ìˆ˜ ë§ì€ ì˜êµ¬ì‹¬ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "hidden sizeê°€ ì™œ í•„ìš”í•˜ì§€..? ë“± \n",
    "\n",
    "ìš°ì„ ì€ NLP ìì²´ì— ëŒ€í•œ ë§¥ë½ì„ ì´í•´í•˜ê³ , í•˜ë‚˜ì”© ë°°ì›Œë‚˜ê°€ë ¤ í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### 3-2-2. val-loss ë¶€ì¡± \n",
    "\n",
    "validation - loss ë¥¼ 2.2 ì´í•˜ë¡œ ë–¨ì–´ëœ¨ë¦¬ëŠ” ê²ƒì´ ì–´ë ¤ì› ìŠµë‹ˆë‹¤.  \n",
    "ì•„ì§ val-lossê°€ ë¬´ì—‡ì„ í†µí•´ ì¤„ì–´ë“œëŠ” ê²ƒì¸ì§€ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.  \n",
    "ì €ëŠ” ë‹¤ì–‘í•œ ì‹œë„ë¥¼ í•´ë´¤ì§€ë§Œ, ê²°ì •ì ì¸ ì˜í–¥ì„ ë¼ì¹œ ê²ƒì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- embedding size\n",
    "- hidden size \n",
    "\n",
    "ì´ ë‘ ì†ì„±ì˜ ê°’ì„ ê°ê° 512 ì™€ 2048ë¡œ í‚¤ì› ë”ë‹ˆ val-loss ì˜ ê°’ì´ ìœ ì˜ë¯¸í•˜ê²Œ ë‚´ë ¤ê°”ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 3-3. ì¶”í›„  \n",
    "\n",
    "í‰ì†Œ ë¬¸í•™ì´ë‚˜ ê¸€ì„ ì¢‹ì•„í•˜ëŠ”ì§€ë¼ ì¸ê³µì§€ëŠ¥ ì–¸ì–´ ëª¨ë¸ì´ í¥ë¯¸ë¡œì› ìŠµë‹ˆë‹¤. ê²¬ë¬¸ì„ ë„“í˜€ í•œêµ­ì–´ NLP ë¥¼ ì‹œë„í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf26f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
