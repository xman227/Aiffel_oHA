{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb4eb39",
   "metadata": {},
   "source": [
    "# [E-04] To make lyricist "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69470caf",
   "metadata": {},
   "source": [
    "## Contexts\n",
    "\n",
    "### 1. READY\n",
    "    1-1 오늘의 Exp와 Rubric  \n",
    "    1-2 사용하는 라이브러리  \n",
    "\n",
    "### 2. GAME\n",
    "    2-1. 데이터 읽어오기  \n",
    "    2-2. 데이터 전처리\n",
    "        2-2-1. Tokenize (문장의 정형화)\n",
    "        2-2-2. Tensor (리스트 객체를 텐서 객체로 변환)\n",
    "        2-2-3. 데이터 분리 (Train, Validation)\n",
    "        2-2-4. 데이터셋 화 (텐서 객체를 dataset 객체로 변환)\n",
    "    2-3. 모델 학습  \n",
    "    2-4. 데이터 평가   \n",
    "\n",
    "### 3. POTG\n",
    "    3-1. 소감(POTG)  \n",
    "    3-2. 어려웠던 점과 극복방안  \n",
    "    3-3. 추후  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9e9b7",
   "metadata": {},
   "source": [
    "## 1. READY\n",
    "\n",
    "   ### 1-1. 오늘의 EXP 와 Rubric\n",
    "\n",
    "오늘의 EXP 내용은 NLP (Natural Language Processing) 기술을 이용한 인공지능 작사 모델 생성이다.\n",
    "\n",
    "- 데이터 : 미국 유명 아티스트 50명들의 곡 가사정보를 담은 텍스트 파일\n",
    "<img src=\"./img/lyric.PNG\" width=\"200px\"></img>\n",
    "\n",
    "\n",
    "- 모델 : tensor - Adam\n",
    "\n",
    "- rubric 제시\n",
    "\n",
    "|평가문항|상세기준|\n",
    "|---|---|\n",
    "|1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?|텍스트 제너레이션 결과가 그럴듯한 문장으로 생서되는지|\n",
    "|2. 데이터의 전처리/ 데이터셋 구성 과정이 체계적으로 진행되는가?| 특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되는지|\n",
    "|3. 텍스트 생성모델이 잘 학습되었는지|텍스트 생성모델의 validation loss가 2.2 이하인지|\n",
    "\n",
    "### 1-2. 사용하는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b351bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf #tf.keras.preprocessing.text.Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d757f",
   "metadata": {},
   "source": [
    "- glob : 파일 핸들링 라이브러리\n",
    "- os : 경로 설정 라이브러리\n",
    "- re : 정규표현식 라이브러리\n",
    "- tf : 인공지능 모델 생성, 학습, 저장 라이브러리  \n",
    "  tf.keras.preprocessing.text.Tokenizer :(토큰화 / 단어사전 생성 / 숫자변환) 벡터화\n",
    "- train_test_split : 모은 데이터를 분리하는 라이브러리\n",
    "- np / pandas : 데이터 구성을 이루는 라이브러리\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f2713",
   "metadata": {},
   "source": [
    "# 2. GAME\n",
    "## 2-1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8821aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel//Study/Exp_4_lyric/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d506f",
   "metadata": {},
   "source": [
    "자 리스트 안에 문장별로 쭉 str데이터가 들어갔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50a55f",
   "metadata": {},
   "source": [
    "## 2-2. 데이터 전처리\n",
    "### 2-2-1. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3140f1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Now', \"I've\", 'heard', 'there', 'was', 'a', 'secret', 'chord']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1 소문자화\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2 특수문자 양쪽에 공백\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3 여러 공백을 하나의 공백으로\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4 알파벳, ?.!,¿가 아닌 모든 문자를 하나의 공백으로\n",
    "    sentence = sentence.strip() # 5 양쪽 공백 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6 \n",
    "    return sentence\n",
    "\n",
    "raw_corpus[0].split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc741d87",
   "metadata": {},
   "source": [
    "preprocess_sentence() 함수로 데이터를 정형화 시켜야 한다.\n",
    "\n",
    "* preprocess_sentence() 는  \n",
    "  - 문장부호를 띄워 놓기\n",
    "  - 전부 소문자화\n",
    "  - 특수문자 제거\n",
    "  - start , end 추가  \n",
    "  \n",
    "를 통해 모든 문장을 동일한 규칙성을 띠게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d3a050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19736 개는 너무 길어서 제외\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156013"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "\n",
    "excepted = 0  # 제외되는 양\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    if len(preprocessed_sentence.split()) > 15:\n",
    "        excepted += 1\n",
    "        continue\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "print(excepted, '개는 너무 길어서 제외')\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03678849",
   "metadata": {},
   "source": [
    "\n",
    "만든 함수를 이용하기 전\n",
    "- 아무것도 없는 줄\n",
    "- : 로 끝나는 문장(가수나 상대가수의 이름이 있다고 추측)  \n",
    "\n",
    "을 제거한 다음 메서드로 정형화를 시켰다, 그리고 정제된 문장 중  \n",
    "단어가 15개를 넘는 문장들(start, end 포함) 을 제하고 문장리스트를 만들었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883c0af",
   "metadata": {},
   "source": [
    "### 2-2-2. Tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0b539",
   "metadata": {},
   "source": [
    "우리의 데이터를 학습시키기 위해서는 우리의 tokenizer 데이터를  \n",
    "tensor 객체로 변환시켜주어야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e305e780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f22905e3f40>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efd040c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   50,    4,   95,  303,   62,   53,    9,  946, 6263,    3,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455e041",
   "metadata": {},
   "source": [
    "그 객체와 함께 쓰이는 문장사전(12000개로 구성)을 Tokenizer 메서드를 통해 불렀고,  \n",
    "우리가 모은 문장 리스트를 통해 필요한 문장을 사전에 구축하였다.\n",
    "\n",
    "추가로\n",
    "- tokenizer.texts_to_sequences 메서드를 통해 우리의 문장 리스트를 텐서화 했다.  \n",
    "- tf.keras.preprocessing.sequence.pad_sequences메서드를 통해 문장별 token 수를 맞춘다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53c867",
   "metadata": {},
   "source": [
    "텐서의 src , target 데이터를 다음과 같다.\n",
    "\n",
    "src 인스턴스는 엔딩이 없는 시작 데이터 (문제지) 이고  \n",
    "tgt 인스턴스는 스타트가 없는 마무리 데이터 (답안지) 이다.\n",
    "\n",
    "이 두 데이터를 연결지으며 다음에 올 문장을 예측한다.\n",
    "\n",
    "**이것이 RNN 의 기본 메커니즘이다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58375889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1760a",
   "metadata": {},
   "source": [
    "### 2-2-3. 데이터셋을 train, validation 값으로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354cf05",
   "metadata": {},
   "source": [
    "NLP 는 목적성(새로운 작사를 하려는 의도) 에 따라 test 값으로 측정하기가 애매하다.  \n",
    "그러므로 우리에게 필요한 데이터 구성은\n",
    "\n",
    "- 데이터를 학습시키는 Train 데이터\n",
    "- 데이터를 학습시키며 하이퍼 파라미터를 조정해주는 Validation 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84d0838",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6222e0",
   "metadata": {},
   "source": [
    "sklearn 모듈의 train_test_split 메서드로 0.2의 비율로 분리했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae29071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d5a9d",
   "metadata": {},
   "source": [
    "위와 같이 14개의 token을 가진 124810개의 문장으로 구성되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5b5e5",
   "metadata": {},
   "source": [
    "### 2-2-4. dataset\n",
    "\n",
    "모델 학습 직전 그 모델에 맞도록 데이터에 일정 파라미터 값을 설정하고 변환시켜야 한다.\n",
    "\n",
    "1. buffer size : 넣을 데이터의 양 (124810)\n",
    "2. batch size : 수행할 미니배치의 크기 (256개씩 묶을 것이다)\n",
    "3. steps per epoch : epoch마다 학습할 횟수 = (버퍼사이즈를 미니배치로 나눈 만큼)\n",
    "\n",
    "* tf.data.Dataset.from_tensor_slices 메서드를 통해  \n",
    "tensor에서 dataset 으로 데이터를 변환한다.\n",
    "\n",
    "\n",
    "* shuffle 메서드와 batch 메서드로 설정한 파라미터값을 넣어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82827eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487\n",
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "print(steps_per_epoch)\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)) #이걸로 tensor의 train 데이터를 tf.data.dataset 객체로 변환하여 사용할거임\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)) #이걸로 tensor의 validation 데이터를 tf.data.dataset 객체로 변환하여 사용할거임\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578334d",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2368c",
   "metadata": {},
   "source": [
    "## 2-3. 모델 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "566e0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c259e7f",
   "metadata": {},
   "source": [
    "TextGenerator 클래스는 우리가 만들 모델의 작동원리를 나타낸다 .  \n",
    "내부에는 임베딩 레이어, 2개의 RNN 레이어, 1개의 Dense 레이어가 있다.  \n",
    "\n",
    "- embedding size : 워드 벡터의 차원 수  \n",
    "- hidden size : LSTM레이어의 hidden state 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53779e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_31/1230560086.py:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Epoch 1/10\n",
      "487/487 [==============================] - 289s 518ms/step - loss: 3.2777 - val_loss: 2.9099\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 262s 537ms/step - loss: 2.7318 - val_loss: 2.6511\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 262s 538ms/step - loss: 2.4125 - val_loss: 2.4685\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 262s 537ms/step - loss: 2.1014 - val_loss: 2.3304\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 261s 536ms/step - loss: 1.8100 - val_loss: 2.2380\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 261s 536ms/step - loss: 1.5535 - val_loss: 2.1693\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 262s 537ms/step - loss: 1.3446 - val_loss: 2.1385\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 261s 536ms/step - loss: 1.1872 - val_loss: 2.1399\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 261s 536ms/step - loss: 1.0818 - val_loss: 2.1555\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 262s 538ms/step - loss: 1.0219 - val_loss: 2.1767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21ec2459d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d2dda",
   "metadata": {},
   "source": [
    "학습이 완료되었습니다  \n",
    "최종 결과로는 loss : 1.34, val-loss : 2.14 를 채택할 수 있다! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693538d",
   "metadata": {},
   "source": [
    "### 2-4. 데이터 평가\n",
    "말한 바와 같이 NLP 의 경우에는 테스트 데이터가 따로 없기에  \n",
    "만들어진 모델로 직접 글을 작성하여 마음에 드는지를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d61e3a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190a503",
   "metadata": {},
   "source": [
    "모델에서는 tensor 데이터를 사용하기 때문에  \n",
    "변수를 입력하고 데이터를 빼올 때 항상 데이터객체 전환과정이 필요하다. \n",
    "\n",
    "위는 그 기능과 함께 최대 20단어까지 나오도록 설정한 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc44cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so much , so o o <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5fa94",
   "metadata": {},
   "source": [
    "너무 로맨틱하다, 작사에 맞게 작곡하고 싶은 욕구가 샘솟는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df8833bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a monster <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f38aae",
   "metadata": {},
   "source": [
    "어떤 의미의 괴물일지에 대한 궁금증이 등을 타고 흐른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "902fa74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3c682",
   "metadata": {},
   "source": [
    "나도 내 곁을 맴도는 그녀가 있었으면 좋겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2575a248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c581a228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> finished work , i m gonna watch this motherfuckin thing <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> finished work ,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d48a3a",
   "metadata": {},
   "source": [
    "난 다 했고, 이제 빌어먹을 넷플릭스 보러간다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f15f99",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78dda4",
   "metadata": {},
   "source": [
    "\n",
    "## 3. POTG\n",
    "\n",
    "### 3-1. 소감(POTG)  \n",
    "#### \" 🙄 나, NLP에 재능이 있어버릴지도..?\" \n",
    "CV와는 다른 접근방식에 흥미를 느꼈습니다! 그런데 결국 가장 잘 나올 것 같은 말을 통계적으로 찾아 붙이는 거라면 독특하고 창의적인 가사는 나오기 힘든 것이 아닐까요?\n",
    "\n",
    "### 3-2. 어려웠던 점과 극복방안  \n",
    "\n",
    "#### 3-2-1. NLP 모델 구조의 심도없는 이해\n",
    "\n",
    "우리가 오늘 만든 모델은 임베딩 레이어 한 개, LSTM 레이어 두 개,  \n",
    "Danse 레이어 한 개로 이루어져 있습니다.\n",
    "\n",
    "<img src=\"./img/rnn2.PNG\"></img>\n",
    "\n",
    "하지만 저는 아직 RNN 이 뭔지, 왜 DANSE 모델을 LInear 라 하는지,  \n",
    "모르는 것 투성입니다.\n",
    "\n",
    "모델을 이해하지 못한 채 학습을 시키려니, 수 많은 의구심이 들었습니다.\n",
    "\n",
    "hidden size가 왜 필요하지..? 등 \n",
    "\n",
    "우선은 NLP 자체에 대한 맥락을 이해하고, 하나씩 배워나가려 합니다.\n",
    "\n",
    "#### 3-2-2. val-loss 부족 \n",
    "\n",
    "validation - loss 를 2.2 이하로 떨어뜨리는 것이 어려웠습니다.  \n",
    "아직 val-loss가 무엇을 통해 줄어드는 것인지 직관적으로 이해하지 못했습니다.  \n",
    "저는 다양한 시도를 해봤지만, 결정적인 영향을 끼친 것은 아래와 같습니다.\n",
    "\n",
    "- embedding size\n",
    "- hidden size \n",
    "\n",
    "이 두 속성의 값을 각각 512 와 2048로 키웠더니 val-loss 의 값이 유의미하게 내려갔습니다.\n",
    "\n",
    "### 3-3. 추후  \n",
    "\n",
    "평소 문학이나 글을 좋아하는지라 인공지능 언어 모델이 흥미로웠습니다. 견문을 넓혀 한국어 NLP 를 시도해보고 싶습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf26f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
